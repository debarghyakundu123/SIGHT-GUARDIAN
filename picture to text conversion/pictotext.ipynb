{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“« MAKE TEXT\n",
      "STAND OUT FROM\n",
      "BACKGROUNDS 7”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# Path to the Tesseract executable (change this if necessary)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\Debarghya Kundu\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def image_to_text(image_path):\n",
    "    # Open the image file\n",
    "    with Image.open(image_path) as img:\n",
    "        # Perform OCR on the image\n",
    "        text = pytesseract.image_to_string(img)\n",
    "        return text\n",
    "\n",
    "# Example usage:\n",
    "image_path = r'C:\\Users\\Debarghya Kundu\\Desktop\\dream project\\Screenshot_2.png'\n",
    "text = image_to_text(image_path)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“« MAKE TEXT\n",
      "STAND OUT FROM\n",
      "BACKGROUNDS 7”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pyttsx3\n",
    "\n",
    "# Path to the Tesseract executable (change this if necessary)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\Debarghya Kundu\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def image_to_text(image_path):\n",
    "    # Open the image file\n",
    "    with Image.open(image_path) as img:\n",
    "        # Perform OCR on the image\n",
    "        text = pytesseract.image_to_string(img)\n",
    "        return text\n",
    "\n",
    "def speak_text(text):\n",
    "    # Initialize the text-to-speech engine\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    # Set properties (optional)\n",
    "    # engine.setProperty('rate', 150)  # Speed of speech (words per minute)\n",
    "    # engine.setProperty('volume', 0.9)  # Volume (0.0 to 1.0)\n",
    "\n",
    "    # Speak the text\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Example usage:\n",
    "image_path = r'C:\\Users\\Debarghya Kundu\\Desktop\\dream project\\Screenshot_2.png'\n",
    "text = image_to_text(image_path)\n",
    "print(text)  # Print the text extracted from the image\n",
    "speak_text(text)  # Speak out the extracted text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Display the frame\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWebcam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Check for user input to capture image\u001b[39;00m\n\u001b[0;32m     43\u001b[0m key \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import pyttsx3\n",
    "from PIL import Image\n",
    "\n",
    "# Path to the Tesseract executable (change this if necessary)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\Debarghya Kundu\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def image_to_text(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Perform thresholding to enhance text visibility\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Perform OCR on the preprocessed image\n",
    "    text = pytesseract.image_to_string(thresh, lang='eng')  # Change 'eng' to the appropriate language code\n",
    "    return text\n",
    "\n",
    "def speak_text(text):\n",
    "    # Initialize the text-to-speech engine\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    # Set properties (optional)\n",
    "    # engine.setProperty('rate', 150)  # Speed of speech (words per minute)\n",
    "    # engine.setProperty('volume', 0.9)  # Volume (0.0 to 1.0)\n",
    "\n",
    "    # Speak the text\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Webcam', frame)\n",
    "\n",
    "    # Check for user input to capture image\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('c'):  # Press 'c' to capture image\n",
    "        # Perform OCR on the captured image\n",
    "        text = image_to_text(frame)\n",
    "\n",
    "        # Print the extracted text\n",
    "        print(text)\n",
    "\n",
    "        # Speak out the extracted text\n",
    "        speak_text(text)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose an option:\n",
      "1. Live camera text detection\n",
      "2. Upload and read image\n",
      "Enter 'q' to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Choose an option:\n",
      "1. Live camera text detection\n",
      "2. Upload and read image\n",
      "Enter 'q' to quit\n",
      "\n",
      "Choose an option:\n",
      "1. Live camera text detection\n",
      "2. Upload and read image\n",
      "Enter 'q' to quit\n"
     ]
    }
   ],
   "source": [
    "#final code for image, webcam to text detection \n",
    "import cv2\n",
    "import pytesseract\n",
    "import pyttsx3\n",
    "from PIL import Image\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "# Path to the Tesseract executable (change this if necessary)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\Debarghya Kundu\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def image_to_text(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Perform thresholding to enhance text visibility\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Perform OCR on the preprocessed image\n",
    "    text = pytesseract.image_to_string(thresh, lang='eng')  # Change 'eng' to the appropriate language code\n",
    "    return text\n",
    "\n",
    "def speak_text(text):\n",
    "    # Initialize the text-to-speech engine\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    # Speak the text\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def live_camera_text_detection():\n",
    "    # Initialize the webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Live Camera', frame)\n",
    "\n",
    "        # Check for user input to capture image\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('c'):  # Press 'c' to capture image\n",
    "            # Perform OCR on the captured image\n",
    "            text = image_to_text(frame)\n",
    "\n",
    "            # Print the extracted text\n",
    "            print(text)\n",
    "\n",
    "            # Speak out the extracted text\n",
    "            speak_text(text)\n",
    "\n",
    "        # Press 'q' to quit\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the webcam and close all OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def upload_and_read_image():\n",
    "    # Open a file dialog to select image\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename()\n",
    "\n",
    "    # Read the image\n",
    "    if file_path:\n",
    "        image = cv2.imread(file_path)\n",
    "\n",
    "        # Perform OCR on the image\n",
    "        text = image_to_text(image)\n",
    "\n",
    "        # Print the extracted text\n",
    "        print(text)\n",
    "\n",
    "        # Speak out the extracted text\n",
    "        speak_text(text)\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        print(\"Choose an option:\")\n",
    "        print(\"1. Live camera text detection\")\n",
    "        print(\"2. Upload and read image\")\n",
    "        print(\"Enter 'q' to quit\")\n",
    "\n",
    "        option = input(\"Enter your choice: \")\n",
    "\n",
    "        if option == '1':\n",
    "            live_camera_text_detection()\n",
    "        elif option == '2':\n",
    "            upload_and_read_image()\n",
    "        elif option == 'q':\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid option. Please try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose an option:\n",
      "1. Live camera text detection\n",
      "2. Upload and read image\n",
      "3. Upload and read PDF\n",
      "Enter 'q' to quit\n",
      "\n",
      "Choose an option:\n",
      "1. Live camera text detection\n",
      "2. Upload and read image\n",
      "3. Upload and read PDF\n",
      "Enter 'q' to quit\n",
      "“4 MAKE TEXT\n",
      "STAND OUT FROM\n",
      "BACKGROUNDS ? ~~\n",
      "\n",
      ". j 4 an\n",
      "2 — . a a >\n",
      "jf - vn C ( ~~ NN a\n",
      "\n",
      "\n",
      "Choose an option:\n",
      "1. Live camera text detection\n",
      "2. Upload and read image\n",
      "3. Upload and read PDF\n",
      "Enter 'q' to quit\n"
     ]
    }
   ],
   "source": [
    "#full code for image , pdf , live cam detection model\n",
    "\n",
    "import cv2\n",
    "import pytesseract\n",
    "import pyttsx3\n",
    "from PIL import Image\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Path to the Tesseract executable (change this if necessary)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\Debarghya Kundu\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def image_to_text(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Perform thresholding to enhance text visibility\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Perform OCR on the preprocessed image\n",
    "    text = pytesseract.image_to_string(thresh, lang='eng')  # Change 'eng' to the appropriate language code\n",
    "    return text\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def speak_text(text):\n",
    "    # Initialize the text-to-speech engine\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    # Speak the text\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def live_camera_text_detection():\n",
    "    # Initialize the webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Live Camera', frame)\n",
    "\n",
    "        # Check for user input to capture image\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('c'):  # Press 'c' to capture image\n",
    "            # Perform OCR on the captured image\n",
    "            text = image_to_text(frame)\n",
    "\n",
    "            # Print the extracted text\n",
    "            print(text)\n",
    "\n",
    "            # Speak out the extracted text\n",
    "            speak_text(text)\n",
    "\n",
    "        # Press 'q' to quit\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the webcam and close all OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def upload_and_read_image():\n",
    "    # Open a file dialog to select image\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename()\n",
    "\n",
    "    # Read the image\n",
    "    if file_path:\n",
    "        image = cv2.imread(file_path)\n",
    "\n",
    "        # Perform OCR on the image\n",
    "        text = image_to_text(image)\n",
    "\n",
    "        # Print the extracted text\n",
    "        print(text)\n",
    "\n",
    "        # Speak out the extracted text\n",
    "        speak_text(text)\n",
    "\n",
    "def upload_and_read_pdf():\n",
    "    # Open a file dialog to select PDF\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename()\n",
    "\n",
    "    # Read the PDF\n",
    "    if file_path:\n",
    "        # Extract text from PDF\n",
    "        text = pdf_to_text(file_path)\n",
    "\n",
    "        # Print the extracted text\n",
    "        print(text)\n",
    "\n",
    "        # Speak out the extracted text\n",
    "        speak_text(text)\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        print(\"Choose an option:\")\n",
    "        print(\"1. Live camera text detection\")\n",
    "        print(\"2. Upload and read image\")\n",
    "        print(\"3. Upload and read PDF\")\n",
    "        print(\"Enter 'q' to quit\")\n",
    "\n",
    "        option = input(\"Enter your choice: \")\n",
    "\n",
    "        if option == '1':\n",
    "            live_camera_text_detection()\n",
    "        elif option == '2':\n",
    "            upload_and_read_image()\n",
    "        elif option == '3':\n",
    "            upload_and_read_pdf()\n",
    "        elif option == 'q':\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid option. Please try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
