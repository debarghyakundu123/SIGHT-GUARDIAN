{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8s.pt to 'yolov8s.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21.5M/21.5M [05:21<00:00, 70.3kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Debarghya Kundu\\AppData\\Roaming\\Python\\Python312\\site-packages\\onnxruntime\\capi\\onnxruntime_validation.py:26: UserWarning: Unsupported Windows version (11). ONNX Runtime supports Windows 10 and above, only.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 2 persons, 1 tie, 308.2ms\n",
      "Speed: 7.0ms preprocess, 308.2ms inference, 1739.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 224.6ms\n",
      "Speed: 3.0ms preprocess, 224.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 218.1ms\n",
      "Speed: 3.0ms preprocess, 218.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 261.0ms\n",
      "Speed: 2.7ms preprocess, 261.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tie, 213.1ms\n",
      "Speed: 1.9ms preprocess, 213.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 223.0ms\n",
      "Speed: 2.0ms preprocess, 223.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 224.3ms\n",
      "Speed: 2.0ms preprocess, 224.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 235.9ms\n",
      "Speed: 2.0ms preprocess, 235.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 238.7ms\n",
      "Speed: 2.5ms preprocess, 238.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 366.8ms\n",
      "Speed: 2.0ms preprocess, 366.8ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 318.9ms\n",
      "Speed: 3.4ms preprocess, 318.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 307.8ms\n",
      "Speed: 15.2ms preprocess, 307.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 234.7ms\n",
      "Speed: 3.2ms preprocess, 234.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 226.9ms\n",
      "Speed: 2.0ms preprocess, 226.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 222.9ms\n",
      "Speed: 2.0ms preprocess, 222.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 226.5ms\n",
      "Speed: 1.5ms preprocess, 226.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 217.3ms\n",
      "Speed: 1.0ms preprocess, 217.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 255.2ms\n",
      "Speed: 3.1ms preprocess, 255.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 222.4ms\n",
      "Speed: 3.0ms preprocess, 222.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 231.5ms\n",
      "Speed: 2.0ms preprocess, 231.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 248.8ms\n",
      "Speed: 1.8ms preprocess, 248.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 232.0ms\n",
      "Speed: 2.4ms preprocess, 232.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 tie, 242.6ms\n",
      "Speed: 3.0ms preprocess, 242.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import pyttsx3\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"yolov8s.pt\")\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Create a queue to communicate detected object names\n",
    "object_queue = queue.Queue()\n",
    "\n",
    "# Define the voice function to speak the names of detected objects\n",
    "def voice():\n",
    "    while True:\n",
    "        # Get the list of object names from the queue\n",
    "        object_names = object_queue.get()\n",
    "        \n",
    "        # Check if the sentinel value (None) was sent to stop the function\n",
    "        if object_names is None:\n",
    "            break\n",
    "        \n",
    "        # Convert the list of object names to a single string\n",
    "        detected_objects_str = \", \".join(object_names)\n",
    "        \n",
    "        # Convert the object names to speech\n",
    "        engine.say(f\"Detected {detected_objects_str}\")\n",
    "        engine.runAndWait()\n",
    "\n",
    "# Define the live camera detection function to perform object detection and send object names to the voice function\n",
    "def live_camera_detection():\n",
    "    # Open the default camera (camera index 0)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Check if the camera opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        object_queue.put(None)  # Send sentinel value to stop voice function\n",
    "        return\n",
    "\n",
    "    # Process video frames in a loop\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame.\")\n",
    "            break\n",
    "        \n",
    "        # Perform object detection using the YOLO model\n",
    "        results = model.predict(frame, conf=0.5)\n",
    "\n",
    "        # Display the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "        \n",
    "        # Create a set to store unique object names detected in the frame\n",
    "        detected_objects = set()\n",
    "        \n",
    "        # Read out the names of detected objects\n",
    "        for detection in results:\n",
    "            # Iterate through each bounding box in the detection result\n",
    "            for box in detection.boxes:\n",
    "                # Access the class ID from the detection (box) object\n",
    "                class_id = int(box.cls)\n",
    "                \n",
    "                # Use the class ID to get the object name from the model's names dictionary\n",
    "                object_name = model.names[class_id]\n",
    "                \n",
    "                # Add the object name to the set of detected objects\n",
    "                detected_objects.add(object_name)\n",
    "\n",
    "        # Convert the set of detected objects to a list\n",
    "        detected_objects_list = list(detected_objects)\n",
    "\n",
    "        # Only put the detected objects in the queue if the queue is empty\n",
    "        if object_queue.empty():\n",
    "            object_queue.put(detected_objects_list)\n",
    "\n",
    "        # Show the annotated frame\n",
    "        cv2.imshow(\"Live Camera Detection\", annotated_frame)\n",
    "\n",
    "        # Add a sleep interval to control the detection rate and allow the voice function to catch up\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        # Break the loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the camera and close all OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Send sentinel value to stop voice function\n",
    "    object_queue.put(None)\n",
    "\n",
    "# Run the live camera detection function and voice function concurrently\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a thread for the voice function\n",
    "    voice_thread = threading.Thread(target=voice)\n",
    "    \n",
    "    # Start the voice thread\n",
    "    voice_thread.start()\n",
    "    \n",
    "    # Run the live camera detection function\n",
    "    live_camera_detection()\n",
    "    \n",
    "    # Wait for the voice thread to finish\n",
    "    voice_thread.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
